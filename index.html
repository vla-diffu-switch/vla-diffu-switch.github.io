<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control of an Anthropomorphic Hand</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>
  




<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control <br> of an Anthropomorphic Hand
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Cheng Pan<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/kaijunge" style="color: black; text-decoration: none;">Kai Junge</a><sup>1</sup>,
            <span class="author-block">
              Josie Hughes<sup>1</sup>,</span>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>EPFL,</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!--
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              -->
              <span class="link-block">
                <a href="https://cyber-alligater.github.io/vla_diff_web/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=s8aLcmAqhRI"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://cyber-alligater.github.io/vla_diff_web/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code coming soon</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            To advance autonomous dexterous manipulation, we propose a hybrid control method that combines the relative advantages 
            of a fine-tuned Vision-Language-Action (VLA) model and diffusion models. 
          </p>
          <p>
            The VLA model provides language commanded high-level planning, which is highly generalizable,
            while the diffusion model handles low-level interactions which offers the precision and robustness 
            required for specific objects and environments. By incorporating a switching signal into the 
            training-data, we enable event based transitions between these two models for a pick-and-place task
            where the target object and placement location is commanded through language. 
            This approach is deployed on our anthropomorphic ADAPT Hand 2, a 13DoF robotic hand, 
            which incorporates compliance through series elastic actuation allowing for resilience for any interactions: 
            showing the first use of a multi-fingered hand controlled with a VLA model. 
          </p>
          <p>
            We demonstrate this model switching approach results in a over 80% success rate compared to under 40% when only 
            using a VLA model, enabled by accurate near-object arm motion by the VLA model and a multi-modal grasping 
            motion with error recovery abilities from the diffusion model.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/s8aLcmAqhRI?si=U3cDIu-FXDrDDIIP" 
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>
  

<section class="section no_top_pad">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <img src="./static/images/overview.png"    alt=""  style="display: block; width: 95%; margin-left: auto; margin-right: auto" >
          <p>            
            (a) Combined VLA and Diffusion policy approach for dexterous manipulation which uses an event signal σ to transition between the different models, 
             enabling text input to be translated to hand and wrist commands for a anthropomorphic manipulator. 
            (b) Depiction of the concept to switch between the VLA and diffusion model using a 
            common event signal σ that tracks key moments in the pick-and-place task.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section no_top_pad">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Teleoperation</h2>
        <div class="content has-text-justified">
          <img src="./static/images/teleop_adapt.png" style="width: 100%; margin-bottom: 15px; margin-top: 10px;" class="is-centered" >
          <p>            
            (a) Left) Robot setup for gathering training-data through teleoperation, showing the use of the Vision pro, 
            and the location of the two cameras for capturing training data. Right) The test objects and environment used for data-capture and testing. 
            (b) ADAPT Hand 2, highlighting the soft continuous skin, compliant series elastic finger joints, and the anatomically driven design.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section no_top_pad">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model Training</h2>
        <div class="content has-text-justified">
          <img src="./static/images/training_model.png"   alt=""  style="display: block; width: 75%; margin-left: auto; margin-right: auto" >
          <p>            
            The VLA model is fine-tuned based the pre-trained openVLA model on the dataset excluding the grasping periods. 
            Instead of using images from a single camera as shown originally, we combine two images from cam1 and cam2. 
            The two images are resized to 224x144 ad 224x80, to then be vertically concatenated into a single image. 
            The fine-tuning continues until the training action accuracy exceed 95% and converges. 
            The fine-tuning is loaded on a cluster virtual machine with a single A100-80GB. 
            The training of diffusion policy model is based on the collected grasping demonstration. 
            The model is trained for 1500 epochs on a custom GPU.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3">VLA model </h2>
    <h3 class="title is-4">bring robot hand to target object</h3>

    <div class="columns is-centered">
      
      <!-- VLA red pepper -->
      <div class="column">
        <div class="content">
          <!--
          <h2 class="title is-3">  xxxxxxx  </h2>
          -->
          <p>
            Language: Put <b>Red Pepper</b> on Yellow Plate
          </p>
          <video id="vla-redpepper" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/vla_arm_redPepper.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ VLA red pepper -->

      <!-- VLA tape -->
      <div class="column">
        <!--         
        <h2 class="title is-3"> xxxx </h2> 
        -->
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Language: Put <b>Tape</b> on Purple Plate
            </p>
            <video id="vla-tape" controls muted playsinline height="100%">
              <source src="./static/videos/vla_arm_tape.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ VLA tape -->
</div>


<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3">Diffusion Policy </h2>
    <h3 class="title is-4">grasp object</h3>

    <div class="columns is-centered">
      
      <!-- Diffu red pepper -->
      <div class="column">
        <div class="content">
          <video id="diffu-redpepper" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/diffu_redPepper.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Diffu red pepper -->

      <!-- Diffu tape -->
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video id="diffu-tape" controls muted playsinline height="100%">
              <source src="./static/videos/diffu_tape.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Diffu tape -->

    
    <h3 class="title is-4">multi-modal behavior</h3>

    <div class="columns is-centered">
      
      <!-- Diffu multi-modal 1 -->
      <div class="column">
        <div class="content">
          <video id="diffu-multi1" controls muted loop playsinline height="100%">
            <source src="./static/videos/multi-modal_blueblock.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Diffu multi-modal 1 -->

      <!-- Diffu multi-modal 2 -->
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video id="diffu-multi2" controls muted playsinline height="100%">
              <source src="./static/videos/multi-modal_tape.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Diffu multi-modal 2 -->


<h3 class="title is-4">recovery behavior</h3>

    <div class="columns is-centered">
      
      <!-- Diffu recovery -->
      <div class="column">
        <div class="content">
          <video id="diffu-multi1" controls muted loop playsinline style="width: 60%; height: auto;">
            <source src="./static/videos/diffu_recv_bluePaper.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
      <!--/ Diffu recovery -->
</div>
</section>
    

<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3">Combined VLA & Diffusion model</h2>
<!--     <h3 class="title is-4">bring robot hand to target object</h3> -->

    <div class="columns is-centered">
      
      <!-- VLA red pepper -->
      <div class="column">
        <div class="content">
          <video id="vla-redpepper" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Bluepaper_combined.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ VLA red pepper -->

</div>



    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
